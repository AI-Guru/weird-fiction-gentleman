{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied: progressbar2 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (3.38.0)\n",
      "Requirement already satisfied: six in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from progressbar2) (1.11.0)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from progressbar2) (2.3.0)\n",
      "Requirement already satisfied: keras in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (2.2.0)\n",
      "Requirement already satisfied: pyyaml in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from keras) (3.13)\n",
      "Requirement already satisfied: keras-applications==1.0.2 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from keras) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from keras) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: h5py in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: keras-preprocessing==1.0.1 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from keras) (1.0.1)\n",
      "Requirement already satisfied: tensorflow in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (1.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: setuptools<=39.1.0 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from tensorflow) (39.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: tensorboard<1.10.0,>=1.9.0 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from tensorflow) (1.9.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from tensorflow) (0.3.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from tensorboard<1.10.0,>=1.9.0->tensorflow) (2.6.11)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/site-packages (from tensorboard<1.10.0,>=1.9.0->tensorflow) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install progressbar2\n",
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristanbehrens/Development/python-venvs/venv-3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tristanbehrens/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /Users/tristanbehrens/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import urllib\n",
    "from collections import Counter\n",
    "import html\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('perluniprops')\n",
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "import random\n",
    "import progressbar\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "\n",
    "# This tokenizer is nice, but could cause problems.\n",
    "try:\n",
    "    from nltk.tokenize.moses import MosesDetokenizer\n",
    "    detokenizer = MosesDetokenizer()\n",
    "    use_moses_detokenizer = True\n",
    "except:\n",
    "    use_moses_detokenizer = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters.\n",
    "\n",
    "Note: As a start you could reduce the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus parameters.\n",
    "download_anyway = False\n",
    "corpus_url = \"https://archive.org/stream/TheCollectedWorksOfH.p.Lovecraft/The-Collected-Works-of-HP-Lovecraft_djvu.txt\"\n",
    "corpus_path = \"lovecraft.txt\"\n",
    "\n",
    "# Preprocessing parameters.\n",
    "preprocess_anyway = False\n",
    "preprocessed_corpus_path = \"lovecraft_preprocessed.p\"\n",
    "most_common_words_number = 10000\n",
    "\n",
    "# Training parameters.\n",
    "train_anyway = False\n",
    "model_path = \"lovecraft_model.h5\"\n",
    "dataset_size = 5000\n",
    "sequence_length = 30\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "hidden_size = 1000\n",
    "\n",
    "# Generation parameters.\n",
    "generated_sequence_length = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(sequence, vocabulary):\n",
    "    \"\"\" Encodes a sequence of tokens into a sequence of indices. \"\"\"\n",
    "\n",
    "    return [vocabulary.index(element) for element in sequence if element in vocabulary]\n",
    "\n",
    "\n",
    "def decode_indices(indices, vocabulary):\n",
    "    \"\"\" Decodes a sequence of indices and returns a string. \"\"\"\n",
    "\n",
    "    decoded_tokens = [vocabulary[index] for index in indices]\n",
    "    if use_moses_detokenizer  == True:\n",
    "        return detokenizer.detokenize(decoded_tokens, return_str=True)\n",
    "    else:\n",
    "        return \" \".join(decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the corpus.\n",
    "\n",
    "Note: The corpus will not be downloaded if it is already on the drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus already downloaded.\n"
     ]
    }
   ],
   "source": [
    "def download_corpus_if_necessary():\n",
    "    \"\"\"\n",
    "    Downloads the corpus either if it is not on the hard-drive or of the\n",
    "    download is forced.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(corpus_path) or download_anyway == True:\n",
    "        print(\"Downloading corpus...\")\n",
    "\n",
    "        # Dowloading content.\n",
    "        corpus_string = urllib.request.urlopen(corpus_url).read().decode('utf-8')\n",
    "\n",
    "        # Removing HTML-stuff.\n",
    "        index = corpus_string.index(\"<pre>\")\n",
    "        corpus_string = corpus_string[index + 5:]\n",
    "        index = corpus_string.find(\"</pre>\")\n",
    "        corpus_string = corpus_string[:index ]\n",
    "        corpus_string = html.unescape(corpus_string)\n",
    "\n",
    "        # Write to file.\n",
    "        corpus_file = open(corpus_path, \"w\")\n",
    "        corpus_file.write(corpus_string)\n",
    "        corpus_file.close()\n",
    "\n",
    "        print(\"Corpus downloaded to\", corpus_path)\n",
    "    else:\n",
    "        print(\"Corpus already downloaded.\")\n",
    "\n",
    "download_corpus_if_necessary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the corpus.\n",
    "\n",
    "Note: The corpus will not be preprocessed if this has been done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus already preprocessed.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_corpus_if_necessary():\n",
    "    \"\"\"\n",
    "    Preprocesses the corpus either if it has not been done before or if it is\n",
    "    forced.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(preprocessed_corpus_path) or preprocess_anyway == True:\n",
    "        print(\"Preprocessing corpus...\")\n",
    "\n",
    "        # Opening the file.\n",
    "        corpus_file = open(corpus_path, \"r\")\n",
    "        corpus_string = corpus_file.read()\n",
    "\n",
    "        # Getting the vocabulary.\n",
    "        print(\"Tokenizing...\")\n",
    "        corpus_tokens = word_tokenize(corpus_string)\n",
    "        print(\"Number of tokens:\", len(corpus_tokens))\n",
    "        print(\"Building vocabulary...\")\n",
    "        word_counter = Counter()\n",
    "        word_counter.update(corpus_tokens)\n",
    "        print(\"Length of vocabulary before pruning:\", len(word_counter))\n",
    "        vocabulary = [key for key, value in word_counter.most_common(most_common_words_number)]\n",
    "        print(\"Length of vocabulary after pruning:\", len(vocabulary))\n",
    "\n",
    "        # Converting to indices.\n",
    "        print(\"Index-encoding...\")\n",
    "        indices = encode_sequence(corpus_tokens, vocabulary)\n",
    "        print(\"Number of indices:\", len(indices))\n",
    "\n",
    "        # Saving.\n",
    "        print(\"Saving file...\")\n",
    "        pickle.dump((indices, vocabulary), open(preprocessed_corpus_path, \"wb\"))\n",
    "    else:\n",
    "        print(\"Corpus already preprocessed.\")\n",
    "\n",
    "preprocess_corpus_if_necessary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trains the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (500 of 500) |######################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the dataset...\n",
      "Generating data set...\n",
      "Creating model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 1000)          10000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000)              8004000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10000)             10010000  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10000)             0         \n",
      "=================================================================\n",
      "Total params: 28,014,000\n",
      "Trainable params: 28,014,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Compiling model...\n",
      "Training model...\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 9.1998 - categorical_accuracy: 0.0620\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 7.7014 - categorical_accuracy: 0.0780\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 5.6774 - categorical_accuracy: 0.0440\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 5.3507 - categorical_accuracy: 0.0280\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 5.2840 - categorical_accuracy: 0.0560\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 5.1928 - categorical_accuracy: 0.0660\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 5.1603 - categorical_accuracy: 0.0780\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 5.1146 - categorical_accuracy: 0.0780\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 5.1022 - categorical_accuracy: 0.1020\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 5.0843 - categorical_accuracy: 0.0920\n",
      "dict_keys(['loss', 'categorical_accuracy'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_neural_network():\n",
    "    \"\"\"\n",
    "    Trains the corpus either if it has not been done before or if it is\n",
    "    forced.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(model_path) or train_anyway == True:\n",
    "\n",
    "        # Loading index-encoded corpus and vocabulary.\n",
    "        indices, vocabulary = pickle.load(open(preprocessed_corpus_path, \"rb\"))\n",
    "\n",
    "        # Get the dataset.\n",
    "        print(\"Getting the dataset...\")\n",
    "        data_input, data_output = get_dataset(indices)\n",
    "        data_output = utils.to_categorical(data_output, num_classes=len(vocabulary))\n",
    "\n",
    "        # Creating the model.\n",
    "        print(\"Creating model...\")\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Embedding(len(vocabulary), hidden_size, input_length=sequence_length))\n",
    "        model.add(layers.LSTM(hidden_size))\n",
    "        model.add(layers.Dense(len(vocabulary)))\n",
    "        model.add(layers.Activation('softmax'))\n",
    "        model.summary()\n",
    "\n",
    "        # Compining the model.\n",
    "        print(\"Compiling model...\")\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['categorical_accuracy']\n",
    "        )\n",
    "\n",
    "        # Training the model.\n",
    "        print(\"Training model...\")\n",
    "        history = model.fit(\n",
    "            data_input, data_output,\n",
    "            epochs=epochs, batch_size=batch_size)\n",
    "        model.save(model_path)\n",
    "        plot_history(history)\n",
    "\n",
    "        \n",
    "def get_dataset(indices):\n",
    "    \"\"\" Gets a full dataset of a defined size from the corpus. \"\"\"\n",
    "\n",
    "    print(\"Generating data set...\")\n",
    "    data_input = []\n",
    "    data_output = []\n",
    "    current_size = 0\n",
    "    bar = progressbar.ProgressBar(max_value=dataset_size)\n",
    "    while current_size < dataset_size:\n",
    "\n",
    "        # Randomly retriev a sequence of tokens and the token right after it.\n",
    "        random_index = random.randint(0, len(indices) - (sequence_length + 1))\n",
    "        input_sequence = indices[random_index:random_index + sequence_length]\n",
    "        output_sequence = indices[random_index + sequence_length]\n",
    "\n",
    "        # Update arrays.\n",
    "        data_input.append(input_sequence)\n",
    "        data_output.append(output_sequence)\n",
    "\n",
    "        # Next step.\n",
    "        current_size += 1\n",
    "        bar.update(current_size)\n",
    "    bar.finish()\n",
    "\n",
    "    # Done. Return NumPy-arrays.\n",
    "    data_input = np.array(data_input)\n",
    "    data_output = np.array(data_output)\n",
    "    return (data_input, data_output)\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    \"\"\" Plots the history of a training. \"\"\"\n",
    "\n",
    "    print(history.history.keys())\n",
    "\n",
    "    # Render the loss.\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(\"history_loss.png\")\n",
    "    plt.clf()\n",
    "\n",
    "    # Render the accuracy.\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(\"history_accuracy.png\")\n",
    "    plt.clf()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "        \n",
    "train_neural_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating texts using trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating texts...\n",
      "Temperature: 0.1898495245781776\n",
      ", the the the the the the the the the , , the the the the the . the the the the the the . the the , the the the , . the the the the , the the the , the the the the the the the the the . the the the the the the the . the the , the , the , the the the the the the the the the the the the the , the the the the the the the the the the the the the the the the . , the the the the , the the the , the the the . the the the the the the , and the , the the , the the the the the the , the . . the the the the the the the the the the the the and the the , . the the the the the , the the , the the , the the the the , the the the the the the the the , the the , , the the . the the the the the , the the the the the , the the the . the the . the the the the the . the the the the the . the the the the the the the the the the the the the , the the the , the the , the the the the the . the the , the , the the the . the the the and the the the the the the , the the the the the the the the , the , . , the , , the the the the the . the the the . the the , the . the the . the , the the the the the the the the the the . the the the the , the the the the the , the the the , the , the the the the , the the the the the the the the the . the the the the the the the the the the the the , the the the the the the the the the the the the , the the the the the , the the the the the . the the the the the the the , the the the the the the , the the the , the , , the the the the the the the the the the the the . the . the the the the , the the the the the the the the the the the the the the the the , the . the which the , , the , the , the the the , the the the the the and the the the the the the the , the the the the the the . the the the the , the the , , the , the , the the the the the , the , the the the the\n",
      "\n",
      "Temperature: 0.432722173151748\n",
      "the . the , the , know , the a . . , the . . the , of the the the the that the . the . . , the the , the the the . the their the . himself the , the the the the . , to the the . relief the the the the the , the the . is the the 's the the came last the the the the , the the , the , the the even , the , the . the than . , the and . The the of , . the is the even the the the the the . the the that the the , , to the and , the . the and and , The and , . , . the to the . the , the , the the the . the ancient , and , the the to the had . . into , at the the the . the the the , had the and . the the when , and the the the , of subdued the party . the , the the of the the the the . , the and , the It the and the the , the and the the the the the the to the . , , questions , relief . the too of the the , the , the , . , in . the , , the the the , . , the even . went the the the . . , the the , the at , the the . , of the the to he he of the the . the the they , , the , of the sunk , the the the the . the his was to . the the the the the the the the the the burning to abode glory the had the the . , the . , the from the the the the . the . of the in the the the which the , was the . the that to and . and was both the . , of the , the he the . Then . and and , the the . the the , had the , burning the the he that the the would and the the the the I the , the , . no to the . of the the and the I the the , . and , and the . the their , and the , the confident the and the the . the , that . the the any of the . . the . the . the the the the of that . , . the would the the the the is the of and the boundaries the the the the , had coast . the the statements the . , , . the the . be the the , the tail , and the the many , , , had the they .\n",
      "\n",
      "Temperature: 0.3385698288292245\n",
      "the . and the the the , I the the , and the the of the , the the the the . the the the the shunned the , the the the the had the the the to the the , the normally the the . the of . the , to the the , the . the . the the . . the , the the the the the , . the , the of the . , the the . , the the was , , the the I . , the and and the the . , the the to , the the had the , the , , the the even , the the . the , the the the the the . . the , . the , the the , and the the the the . the the the the the and and the and the the the the . , the the the . the the the the the the the . the , the the the . the the the the the . the the the the the the , of the the . , . the the the the . the the the the the the the , the the , the . the the , the the night the the the , , the the , the the , the the and the the . I the now the the the the . the , the the the the of the the the the , , the the of the , the and the the the , of the . . the . the , , the the the the the the , , . . and the the the the the the the of the , the the the the the , the , the in the the the . . . , the the the , . the the , the to , the the the . the and , the . nightmare the the the to the that the the the , at to and the it , , the the , the and it The the . the the . the and , the the , the the . the the . the , the , the the . the the the the of the the the , of the the . the the the the the , the the the , , the and the the and the the the the . , the the . the the the the , the the , the of , the when . the the of the and . , the the the of the . . . and the the , , the , , , the the the the and . , the , the , . , the the . the . . the and the , . the the the . , , the I the the the the . the the the the\n",
      "\n",
      "Temperature: 0.8066785243733824\n",
      "been he hill , New , ten , rising forgotten which not warehouse had came manner ye which vaguely jagged the vastly left , subdued discovery altar ten wrought the both , filled the vastly and and the . to the shunned to filled so the - and , . was be to , lore empty Then the eternally on the which , , questions viewed during the quest no party was gazing both he they of ever The the its , , that sunk keep had , , the which the questions the sound with the No I of Sherman . And tail and and quantities city the too the Then . I that vaguely the the night would boundaries . or . , the It , manner Then nightmare the it the it this , the the and he was at . you the , . the things . this . The subsequent , as eternally nightmare inexplicable . . which It , which and to along was last parts which . altar of even . himself the the testing the ancient midnight the cries city . , of eternally vaguely the never , had . imagination the went the the the he farther , tell , the know the the you which it is . Then into trees space even sufficient , effect , strongest quickly outfit , is at Then fragmentary he to and enormous that . not It enormous will filled that to it which let would the the . the it . Poer . any had had . is questions ancient of And himself of the is betwixt relief 's and . he and the that and rock . the nightmare of subdued and oblivion the the and May he gazing while consistent and came of and of the . the - hill , which relief - gone as the burning he no Australia with growing . he their , here the their vastly St. the we . growing their the ever its that the of ancient the , strongest any the earthly nightmare as dread or diary toward He even had viewed the It and eternally fer the Then no subdued toward from the sand the city know by deserted old dread . to , Nyarlathotep at the was which I of of missed . his . in questions , came and not he the by . bad and the into the . bad For and with . do that He now the the . the testing confident the , and is pastimes city been the now . the , quest and a my of Middle singular of my the . along was of , the the . the you of the he , which relief eternally , this he I never of and to , No up rolled I . a present and the missed , flows my and been and had the some when oblivion will , away he vainly ,\n",
      "\n",
      "Temperature: 0.47858890643543706\n",
      ", and the the of and the , . the . , and had the and the the the the the the I and and , the of the of the the which we . , the . , the , the suggestion the , , the , the the the , the , the that . the , rolled the right . the the which the the and the , the the the and fer , relief , of now the . , . . . , the , , , the and of the the the the the it the . a Poer the , the and Then , . , . present , the , With the the the the the the betwixt . the the . . the , fish the the it , the of the human of the the the the the . the , the the , of and the the . the and the of , , the to . the it he Between the . , I the , . the this . and the he the the , the of the . the it the . . . , . . the having the diary is the . , , to the to , the , . . the . and , the , the the , horror the that to the the wondering we I night , the to the person the , the . . . and the he , way the is , and the of . the it the , do the . . had the . the . the and It and the the I , and the than , and the The the the would had the . . of the lines the which , , . the I the of the the been . , of the . the . the the , , of the person the that the that the the the would the , the , . the the . the the . the which . the and the the with the and the the . . the , the and the had , . the to . spent , of of the , . . , the , city not the the the of of , , , the the this and the . , , the , , it the and last the , he the the . the and the a shunned the . that the the , to the the , and is the , the the the and the , the to , had , the the , when he , the to the the . , . the . of questions had that and to to when , the the , of , , the , . the trees , and the , the the the to the and . of the the , with the the the the . thing to , ,\n",
      "\n",
      "Temperature: 0.9790690404171726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guessed and inexplicable had leisurely , flows said of the the as the ' normally the , tail imagination wondering with on Captain ascent hour and village said he and , feel , the oblivion , in testing of the to road or now some from , and trees , that his falls sank `` 's . the the . It of had cut in Between on had went to by on and ? here ye high that the down let down lonely midnight ' and glory as questions any boundaries wild on the us yellow but sand and reflections that , more , relief from came . which slept , earthly down quickly weeping relief things the never sufficient which rock interesting never abode questions the from inexplicable betwixt the sufficient out deserted had the had betwixt they . present viewed the No to left the Captain . boundaries Nyarlathotep the from is the so lore its . there it the and in of sort to the in even they No Saturday would but on flows right thing with grove that you rising ? the had of feel us which the they he James I subject the and I trees Middle is been the out manner night you I grove this he with door earthly he coast a of vastly the a my were I door with went last The at during , the bad went questions , hill and but bad right gone fish it stories . bad the relief and earthly rock ever way with space Australia be to , relief I anxious , Captain they his arriving yellow falls even earthly hour so Then Newport imagination should on now than inexplicable was jagged the was Station thoroughly was of to Australia . any arriving at man guessed grove with tail came wrought right Middle of right the the had me left . when door cries last preliminary 's coast taste out boundaries or on was the Nyarlathotep of space . the never into had yellow lore tail sank , slept as in yellow home no had to a , betwixt I in the leisurely and would Between thither with an of which possibly subject city relief statements and down guessed . the fellows had . the vague his hill of I the let the had filled guessed thing than as more , . not of relief its so carpentry it human , had of Then grotto jagged said . normally , grove bridges the of vaguely party decided farther enormous my stunned rocks of but the correspondence discovery the oblivion The guessed could the Saturday questions I , normally yellow had vaguely the Then to the he burning gazing space sort questions quest of of the vague strongest to singular that out know possibly an of Station left parts the Romero had weeping , that manner normally his questions rolled person a from on the from Then keep toward pastimes Then . guessed sufficient the hideous\n",
      "\n",
      "Temperature: 0.57647895871379\n",
      "the . would the the would I I the the the , so . vainly . the of the the was had the , skill The the , the had , my out the the and the and the May the the the the the sank , and the the , . , warehouse the , the , . . night . I of had the in the , of to last It which to the the relief the so . the . the the came of . the , the the the the their the could . the the the and . an a a I the , earthly the this May vaguely , of and the and he the the the the at , . the the , preliminary . the , . and , thoroughly , , . he the of the and had the of the would . . the even the of growing the and the , . glory , , , had wild had with . the the Then the had it Between the the and came the and the the over the . lore . his up the decided , the the , and and I , the . . the and Olney that out a it the , Nyarlathotep never , had rocks . had in I human . and singular the the . the of the and wild . . horror , my had of the had I the the . the at is , and the sound and the the had clinging of the had a , the , the of by , of in the than the to midnight the my the . the , . my was . . he and . of questions . the possibly of , I correspondence the burning completely , its at . deserted , city the away strongest the of at had . the the we the the , the its the to the the , the he , and the it . was had , the and betwixt , went he the as he had . of the , the and , was and wall their the and . . the a had the the that the skill , , . of . . tell the the to and at which that things reflections , , . , , human enormous It and so the the , of with the and which to is the the and . preliminary and the Nyarlathotep now , the the questions . to the spells the the is the the of No the , the the to the the Norrys the the the the . correspondence the . . at . . betwixt which the the the the the , I . on and . this some the the his be was he its , in , of and . the last the is , had the the the my and not , , on\n",
      "\n",
      "Temperature: 0.6827384853657951\n",
      "the to . the not correspondence of the flows . ever the , night stunned the . boundaries the the the skill to , , to the city the of . , went even and sea the space , entirely never from , the , never the , , . grove than murky even the himself the many the this he would the be `` were on burning out strongest would present the so will . `` the the by Nyarlathotep . The growing it the the the the Nyarlathotep which high . man he questions slept the too . effect , , party the , the , the with the , , the left , and the the of world strongest the The and the this . . , let . . yellow . the Captain grove the rock , . . which me the quantities the their , 's hour . and possibly . . so than and in . the correspondence of . eternally to at ten to , he . the the he the the quest gone the the tell the and I ? world tail had had my ? of the . and and consistent to of - and ' that , , would so skill . and the the present so , the when and leisurely , did would of would . It the on even which . trees I the is it of of Then was and , Negro a the . his even relief so this . had . . the the he , were . the , would of this was the Saturday bridges my the shape to , the , . human . the the that and and . relief the that and a to the on and it . surprising , The the , , I the their . ascent from from the , when a the the had and at which the that here feel the , the which , the , , . to of the the and I . nightmare and and . , . , the . . James we it that the the the relief . . yellow the left , and to the . had guessed during it , hill and The spent , taste the to clinging an and The slept , I he the warehouse and nightmare the quantities , the New Norrys was , to some He man dread with was , , the , had this the at the was a no . of Australia was went , , rolled at their . , into . Negro Then to was now the the . . , the the was is . relief fragmentary Nyarlathotep the . fish that , as tail was . . their sound ? the . , the any . viewed went should the that nightmare the , fellows during , It Saturday , The The the ancient let I subject and . vaguely , stunned\n",
      "\n",
      "Temperature: 0.10079705074111289\n"
     ]
    }
   ],
   "source": [
    "def generate_texts():\n",
    "    \"\"\" Generates a couple of random texts. \"\"\"\n",
    "\n",
    "    print(\"Generating texts...\")\n",
    "\n",
    "    # Getting all necessary data. That is the preprocessed corpus and the model.\n",
    "    indices, vocabulary = pickle.load(open(preprocessed_corpus_path, \"rb\"))\n",
    "    model = models.load_model(model_path)\n",
    "\n",
    "    # Generate a couple of texts.\n",
    "    for _ in range(10):\n",
    "\n",
    "        # Get a random temperature for prediction.\n",
    "        temperature = random.uniform(0.0, 1.0)\n",
    "        print(\"Temperature:\", temperature)\n",
    "\n",
    "        # Get a random sample as seed sequence.\n",
    "        random_index = random.randint(0, len(indices) - (generated_sequence_length))\n",
    "        input_sequence = indices[random_index:random_index + sequence_length]\n",
    "\n",
    "        # Generate the sequence by repeatedly predicting.\n",
    "        generated_sequence = []\n",
    "        while len(generated_sequence) < generated_sequence_length:\n",
    "            prediction = model.predict(np.expand_dims(input_sequence, axis=0))\n",
    "            predicted_index = get_index_from_prediction(prediction[0], temperature)\n",
    "            generated_sequence.append(predicted_index)\n",
    "            input_sequence = input_sequence[1:]\n",
    "            input_sequence.append(predicted_index)\n",
    "\n",
    "        # Convert the generated sequence to a string.\n",
    "        text = decode_indices(generated_sequence, vocabulary)\n",
    "        print(text)\n",
    "        print(\"\")\n",
    "\n",
    "        \n",
    "def get_index_from_prediction(prediction, temperature=0.0):\n",
    "    \"\"\" Gets an index from a prediction. \"\"\"\n",
    "\n",
    "    # Zero temperature - use the argmax.\n",
    "    if temperature == 0.0:\n",
    "        return np.argmax(prediction)\n",
    "\n",
    "    # Non-zero temperature - do some random magic.\n",
    "    else:\n",
    "        prediction = np.asarray(prediction).astype('float64')\n",
    "        prediction = np.log(prediction) / temperature\n",
    "        exp_prediction= np.exp(prediction)\n",
    "        prediction = exp_prediction / np.sum(exp_prediction)\n",
    "        probabilities = np.random.multinomial(1, prediction, 1)\n",
    "        return np.argmax(probabilities)\n",
    "  \n",
    "\n",
    "generate_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
