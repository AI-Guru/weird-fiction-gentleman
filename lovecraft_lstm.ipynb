{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lovecraft lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woebSMG4HMh7"
      },
      "source": [
        "# H.P. Lovecraft Language Generation using LSTM\n",
        "## Dr. Tristan Behrens (https://www.linkedin.com/in/dr-tristan-behrens-734967a2/)\n",
        "\n",
        "Trains a Neural Network on the collected works by H.P. Lovecraft."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT5uzgt8Usza"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import urllib\n",
        "from collections import Counter\n",
        "import html\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('perluniprops')\n",
        "from nltk import word_tokenize\n",
        "import pickle\n",
        "import random\n",
        "import progressbar\n",
        "import glob\n",
        "\n",
        "try:\n",
        "    from nltk.tokenize.moses import MosesDetokenizer\n",
        "    detokenizer = MosesDetokenizer()\n",
        "    use_moses_detokenizer = True\n",
        "except:\n",
        "    use_moses_detokenizer = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7CqLvTPUsza"
      },
      "source": [
        "# Parameters.\n",
        "\n",
        "Note: As a start you could reduce the dataset size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbLZ_-CHUszb"
      },
      "source": [
        "# Corpus parameters.\n",
        "download_anyway = False\n",
        "corpus_path = \"corpus.txt\"\n",
        "\n",
        "# Preprocessing parameters.\n",
        "preprocess_anyway = False\n",
        "preprocessed_corpus_path = \"corpus_preprocessed.p\"\n",
        "most_common_words_number = 10000\n",
        "\n",
        "# Training parameters.\n",
        "train_anyway = False\n",
        "model_path = \"model.h5\"\n",
        "dataset_size = 5000\n",
        "sequence_length = 30\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "hidden_size = 1000\n",
        "\n",
        "# Generation parameters.\n",
        "generated_sequence_length = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NaH1ZdEUszc"
      },
      "source": [
        "# Helpers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r64SBKubUszc"
      },
      "source": [
        "def encode_sequence(sequence, vocabulary):\n",
        "    \"\"\" Encodes a sequence of tokens into a sequence of indices. \"\"\"\n",
        "\n",
        "    return [vocabulary.index(element) for element in sequence if element in vocabulary]\n",
        "\n",
        "\n",
        "def decode_indices(indices, vocabulary):\n",
        "    \"\"\" Decodes a sequence of indices and returns a string. \"\"\"\n",
        "\n",
        "    decoded_tokens = [vocabulary[index] for index in indices]\n",
        "    if use_moses_detokenizer  == True:\n",
        "        return detokenizer.detokenize(decoded_tokens, return_str=True)\n",
        "    else:\n",
        "        return \" \".join(decoded_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIhcO2NXUszd"
      },
      "source": [
        "# Downloading the corpus.\n",
        "\n",
        "Note: The corpus will not be downloaded if it is already on the drive.\n",
        "\n",
        "Note: This would be a fine place to insert your own corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpAu-6WtUszd"
      },
      "source": [
        "def download_corpus_if_necessary():\n",
        "    \"\"\"\n",
        "    Downloads the corpus either if it is not on the hard-drive or of the\n",
        "    download is forced.\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(corpus_path) or download_anyway == True:\n",
        "        \n",
        "        print(\"Downloading corpus...\")\n",
        "        \n",
        "        # Clone the repo.\n",
        "        if not os.path.exists(\"lovecraftcorpus\"):\n",
        "            !git clone https://github.com/vilmibm/lovecraftcorpus\n",
        "            \n",
        "        # Get the files.\n",
        "        paths = glob.glob(\"lovecraftcorpus/*.txt\")\n",
        "        print(paths)\n",
        "        \n",
        "        # Merge all files.\n",
        "        with open(corpus_path, \"w\") as output_file:\n",
        "            for path in paths:\n",
        "                with open(path, \"r\") as input_file:\n",
        "                    output_file.write(input_file.read())\n",
        "                    output_file.write(\"\\n\")\n",
        "           \n",
        "        # Delete repo.\n",
        "        !rm -rf lovecraftcorpus\n",
        "        \n",
        "        print(\"Corpus downloaded to\", corpus_path)\n",
        "    else:\n",
        "        print(\"Corpus already downloaded.\")\n",
        "\n",
        "download_corpus_if_necessary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc_AwFVhUsze"
      },
      "source": [
        "# Preprocessing the corpus.\n",
        "\n",
        "Brings the entire corpus into a format that can be used for training an ANN. Represents the texts as word indices. Includes the vocabulary.\n",
        "\n",
        "Note: The corpus will not be preprocessed if this has been done before.\n",
        "\n",
        "Note: Later you would consider using a real tokenizer here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v7MjnDVUszf"
      },
      "source": [
        "def preprocess_corpus_if_necessary():\n",
        "    \"\"\"\n",
        "    Preprocesses the corpus either if it has not been done before or if it is\n",
        "    forced.\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(preprocessed_corpus_path) or preprocess_anyway == True:\n",
        "        print(\"Preprocessing corpus...\")\n",
        "\n",
        "        # Opening the file.\n",
        "        corpus_file = open(corpus_path, \"r\")\n",
        "        corpus_string = corpus_file.read()\n",
        "\n",
        "        # Getting the vocabulary.\n",
        "        print(\"Tokenizing...\")\n",
        "        corpus_tokens = word_tokenize(corpus_string)\n",
        "        print(\"Number of tokens:\", len(corpus_tokens))\n",
        "        print(\"Building vocabulary...\")\n",
        "        word_counter = Counter()\n",
        "        word_counter.update(corpus_tokens)\n",
        "        print(\"Length of vocabulary before pruning:\", len(word_counter))\n",
        "        vocabulary = [key for key, value in word_counter.most_common(most_common_words_number)]\n",
        "        print(\"Length of vocabulary after pruning:\", len(vocabulary))\n",
        "\n",
        "        # Converting to indices.\n",
        "        print(\"Index-encoding...\")\n",
        "        indices = encode_sequence(corpus_tokens, vocabulary)\n",
        "        print(\"Number of indices:\", len(indices))\n",
        "\n",
        "        # Saving.\n",
        "        print(\"Saving file...\")\n",
        "        pickle.dump((indices, vocabulary), open(preprocessed_corpus_path, \"wb\"))\n",
        "        print(\"Done.\")\n",
        "    else:\n",
        "        print(\"Corpus already preprocessed.\")\n",
        "\n",
        "preprocess_corpus_if_necessary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5asVanCOUszg"
      },
      "source": [
        "# Trains the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix1Wec74H7ir"
      },
      "source": [
        "Creates and traines a Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxczmthnUszh"
      },
      "source": [
        "def train_neural_network():\n",
        "    \"\"\"\n",
        "    Trains the corpus either if it has not been done before or if it is\n",
        "    forced.\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(model_path) or train_anyway == True:\n",
        "\n",
        "        # Loading index-encoded corpus and vocabulary.\n",
        "        indices, vocabulary = pickle.load(open(preprocessed_corpus_path, \"rb\"))\n",
        "\n",
        "        # Get the dataset.\n",
        "        print(\"Getting the dataset...\")\n",
        "        data_input, data_output = get_dataset(indices)\n",
        "        data_output = tf.keras.utils.to_categorical(data_output, num_classes=len(vocabulary))\n",
        "\n",
        "        # Creating the model.\n",
        "        print(\"Creating model...\")\n",
        "        model = tf.keras.models.Sequential()\n",
        "        model.add(tf.keras.layers.Embedding(len(vocabulary), hidden_size, input_length=sequence_length))\n",
        "        model.add(tf.keras.layers.LSTM(hidden_size))\n",
        "        model.add(tf.keras.layers.Dense(len(vocabulary)))\n",
        "        model.add(tf.keras.layers.Activation('softmax'))\n",
        "        model.summary()\n",
        "\n",
        "        # Compining the model.\n",
        "        print(\"Compiling model...\")\n",
        "        model.compile(\n",
        "            loss='categorical_crossentropy',\n",
        "            optimizer='adam',\n",
        "            metrics=['categorical_accuracy']\n",
        "        )\n",
        "\n",
        "        # Training the model.\n",
        "        print(\"Training model...\")\n",
        "        history = model.fit(\n",
        "            data_input, data_output,\n",
        "            epochs=epochs, batch_size=batch_size)\n",
        "        model.save(model_path)\n",
        "        plot_history(history)\n",
        "\n",
        "        \n",
        "def get_dataset(indices):\n",
        "    \"\"\" Gets a full dataset of a defined size from the corpus. \"\"\"\n",
        "\n",
        "    print(\"Generating data set...\")\n",
        "    data_input = []\n",
        "    data_output = []\n",
        "    current_size = 0\n",
        "    bar = progressbar.ProgressBar(max_value=dataset_size)\n",
        "    while current_size < dataset_size:\n",
        "\n",
        "        # Randomly retriev a sequence of tokens and the token right after it.\n",
        "        random_index = random.randint(0, len(indices) - (sequence_length + 1))\n",
        "        input_sequence = indices[random_index:random_index + sequence_length]\n",
        "        output_sequence = indices[random_index + sequence_length]\n",
        "\n",
        "        # Update arrays.\n",
        "        data_input.append(input_sequence)\n",
        "        data_output.append(output_sequence)\n",
        "\n",
        "        # Next step.\n",
        "        current_size += 1\n",
        "        bar.update(current_size)\n",
        "    bar.finish()\n",
        "\n",
        "    # Done. Return NumPy-arrays.\n",
        "    data_input = np.array(data_input)\n",
        "    data_output = np.array(data_output)\n",
        "    return (data_input, data_output)\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\" Plots the history of a training. \"\"\"\n",
        "\n",
        "    print(history.history.keys())\n",
        "\n",
        "    # Render the loss.\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.savefig(\"history_loss.png\")\n",
        "    plt.clf()\n",
        "\n",
        "    # Render the accuracy.\n",
        "    plt.plot(history.history['categorical_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.savefig(\"history_accuracy.png\")\n",
        "    plt.clf()\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "        \n",
        "train_neural_network()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydIBpMHQUszi"
      },
      "source": [
        "# Generating texts using trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn1jLC1qUszi"
      },
      "source": [
        "def generate_texts():\n",
        "    \"\"\" Generates a couple of random texts. \"\"\"\n",
        "\n",
        "    print(\"Generating texts...\")\n",
        "\n",
        "    # Getting all necessary data. That is the preprocessed corpus and the model.\n",
        "    indices, vocabulary = pickle.load(open(preprocessed_corpus_path, \"rb\"))\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "    # Generate a couple of texts.\n",
        "    for _ in range(10):\n",
        "\n",
        "        # Get a random temperature for prediction.\n",
        "        temperature = random.uniform(0.0, 1.0)\n",
        "        print(\"Temperature:\", temperature)\n",
        "\n",
        "        # Get a random sample as seed sequence.\n",
        "        random_index = random.randint(0, len(indices) - (generated_sequence_length))\n",
        "        input_sequence = indices[random_index:random_index + sequence_length]\n",
        "\n",
        "        # Generate the sequence by repeatedly predicting.\n",
        "        generated_sequence = []\n",
        "        while len(generated_sequence) < generated_sequence_length:\n",
        "            prediction = model.predict(np.expand_dims(input_sequence, axis=0))\n",
        "            predicted_index = get_index_from_prediction(prediction[0], temperature)\n",
        "            generated_sequence.append(predicted_index)\n",
        "            input_sequence = input_sequence[1:]\n",
        "            input_sequence.append(predicted_index)\n",
        "\n",
        "        # Convert the generated sequence to a string.\n",
        "        text = decode_indices(generated_sequence, vocabulary)\n",
        "        print(text)\n",
        "        print(\"\")\n",
        "\n",
        "        \n",
        "def get_index_from_prediction(prediction, temperature=0.0):\n",
        "    \"\"\" Gets an index from a prediction. \"\"\"\n",
        "\n",
        "    # Zero temperature - use the argmax.\n",
        "    if temperature == 0.0:\n",
        "        return np.argmax(prediction)\n",
        "\n",
        "    # Non-zero temperature - do some random magic.\n",
        "    else:\n",
        "        prediction = np.asarray(prediction).astype('float64')\n",
        "        prediction = np.log(prediction) / temperature\n",
        "        exp_prediction= np.exp(prediction)\n",
        "        prediction = exp_prediction / np.sum(exp_prediction)\n",
        "        probabilities = np.random.multinomial(1, prediction, 1)\n",
        "        return np.argmax(probabilities)\n",
        "  \n",
        "\n",
        "generate_texts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK9y2aZoUszj"
      },
      "source": [
        "# Thank you!"
      ]
    }
  ]
}